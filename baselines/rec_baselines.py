# -*- coding: utf-8 -*-
"""exCPFair_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nOJpHfPJR9Gbxc5c_fbz-Ycq1j36Swzz

## A package error:
# ImportError: libpython3.7m.so.1.0: cannot open shared object file: No such file or directory
export LD_LIBRARY_PATH=/home/your_username/anaconda3/envs/your_env_name (here: py3)/lib
HERE: export LD_LIBRARY_PATH=/home/srahmani/anaconda3/envs/py3/lib

## Install packages
"""

# Install MIP and Gurobipy for optimization purpose
# ! pip install mip
# ! python -m pip install gurobipy==9.1.2
# install Cornac framework for RecSys
# ! pip install cornac

"""## Config"""

# import packages
import numpy as np
from collections import defaultdict
from tqdm import tqdm
import argparse

from itertools import product
from sys import stdout as out

import cornac
from cornac.eval_methods import BaseMethod
from cornac.models import MostPop, UserKNN, ItemKNN, MF, PMF, BPR, NeuMF, WMF, HPF, CVAE, VAECF, NMF, MLP
from cornac.metrics import Precision, Recall, NDCG, AUC, MAP, FMeasure, MRR, RMSE, MAE
from cornac.data import Reader

### Arguments ###
parser = argparse.ArgumentParser(description='exCPFair')
parser.add_argument('-d','--dataset', help='name of dataset', required=True)
args = parser.parse_args() # Parse the argument

# reading the train, test, and val sets
def read_data(dataset):
  """
  Read the train, test, and tune file using Cornac reader class

  Parameters
  ----------
  dataset : the name of the dataset
    example: 'MovieLens100K'

  Returns
  ----------
  train_data:
    The train set that is 70% of interactions
  tune_data:
    The tune set that is 10% of interactions
  test_data:
    The test set that is 20% of interactions
  """
  reader = Reader()
  train_data = reader.read(fpath=f"datasets/{dataset}/{fold}/train_data.txt", fmt='UIR', sep='\t')
  tune_data = reader.read(fpath=f"datasets/{dataset}/{fold}/validation_data.txt", fmt='UIR', sep='\t')
  test_data = reader.read(fpath=f"datasets/{dataset}/{fold}/test_data.txt", fmt='UIR', sep='\t')
  return train_data, tune_data, test_data

# load data into Cornac evaluation method
def load_data(train_data, validation_data, test_data):
  """
  load data into Cornac evaluation method

  Parameters
  ----------
  train_data: 
    train_data from Reader Class
  test_data:
    test_data from Reader Class

  Returns
  ----------
  eval_method:
    Instantiation of a Base evaluation method using the provided train and test sets
  """
  # exclude_unknowns (bool, default: False) – Whether to exclude unknown users/items in evaluation.
  # Instantiate a Base evaluation method using the provided train and test sets
  eval_method = BaseMethod.from_splits(
      train_data=train_data, test_data=test_data, rating_threshold=0.0, exclude_unknowns=True, verbose=True
  )

  return eval_method

# running the cornac
def run_model(eval_method):
  """
  running the cornac

  Parameters
  ----------
  eval_method: 
    Cornac's evaluation protocol

  Returns
  ----------
  exp:
    Cornac's Experiment
  """
  
  models = [
            MostPop(),
            UserKNN(k=20, similarity="cosine", weighting="bm25", name="UserKNN-BM25"),
            ItemKNN(k=20, similarity="cosine", name="ItemKNN-Cosine"),
            BPR(k=50, max_iter=200, learning_rate=0.001, lambda_reg=0.001, verbose=True),
            # WMF(k=50, max_iter=50, learning_rate=0.001, lambda_u=0.01, lambda_v=0.01, verbose=True, seed=123),
            # MF(k=10, max_iter=25, learning_rate=0.01, lambda_reg=0.02, use_bias=True, seed=123)
            # MLP(
            #     layers=[64, 32, 16, 8],
            #     act_fn="tanh",
            #     learner="adam",
            #     num_epochs=10,
            #     batch_size=256,
            #     lr=0.001,
            #     num_neg=50,
            #     seed=123
            # )
            # HPF(k=50, seed=123, hierarchical=False, name="PF"),
            # VAECF(k=10, autoencoder_structure=[20], act_fn="tanh", likelihood="mult", n_epochs=100, batch_size=100, learning_rate=0.001, beta=1.0, seed=123, use_gpu=True, verbose=True),
            # NeuMF(num_factors=9, layers=[32, 16, 8], act_fn="tanh", num_epochs=5, num_neg=3, batch_size=256, lr=0.001, seed=42, verbose=True)
            ]

  # define metrics to evaluate the models
  metrics = [
            RMSE(), MAE(),
            ]

  # put it together in an experiment, voilà!
  exp = cornac.Experiment(eval_method=eval_method, models=models, metrics=metrics)
  exp.run()

  return exp

# dataset congfig
ds_names = [args.dataset]
folds = [1]

if '/' in ds_names[0]:
  dataset_name = ds_names[0].replace('/', '_')

# results file
fout = open(f"results/results_{dataset_name}.txt", 'w')

# 1: Iterate over the datasets
for dataset in ds_names:
  header_flag = False
  for fold in folds:
    print(f"Datasets: {dataset}")
    # read train, tune, test datasets
    train_data, tune_data, test_data = read_data(dataset=dataset)

    # load data into Cornac and create eval_method
    eval_method = load_data(train_data=train_data, validation_data=tune_data, test_data=test_data)

    # run Cornac models and create experiment object including models' results
    exp = run_model(eval_method=eval_method)

    if header_flag == False:
      metrics = list(exp.result[0].metric_avg_results.keys())
      header = ['Dataset', 'Model', 'Fold'] + metrics
      fout.write("\t".join(str(item) for item in header))
      fout.write('\n')
      header_flag = True
    for r in exp.result:
        results = list(r.metric_avg_results.values())
        results = [round(item, 4) for item in results]
        results = ['Dataset', str(r.model_name), str(0)] + results
        fout.write("\t".join(str(item) for item in results))
        fout.write('\n')

fout.close()

